[swarm]
max_myses = 16

# Ollama providers (local)
[providers.ollama]
endpoint = "http://localhost:11434"
model = "qwen3:8b"
temperature = 0.7
rate_limit = 2.0
rate_burst = 3

[providers.ollama-qwen]
endpoint = "http://localhost:11434"
model = "qwen3:8b"
temperature = 0.7
rate_limit = 2.0
rate_burst = 3

[providers.ollama-qwen-small]
endpoint = "http://localhost:11434"
model = "qwen3:4b"
temperature = 0.7
rate_limit = 2.0
rate_burst = 3

[providers.ollama-llama]
endpoint = "http://localhost:11434"
model = "llama3.1:8b"
temperature = 0.7
rate_limit = 2.0
rate_burst = 3

# OpenCode Zen providers (cloud)
[providers.opencode_zen]
endpoint = "https://opencode.ai/zen/v1"
model = "gpt-5-nano"
temperature = 0.7
rate_limit = 1.0
rate_burst = 2

[providers.zen-nano]
endpoint = "https://opencode.ai/zen/v1"
model = "gpt-5-nano"
temperature = 0.7
rate_limit = 1.0
rate_burst = 2

[providers.zen-pickle]
endpoint = "https://opencode.ai/zen/v1"
model = "big-pickle"
temperature = 0.7
rate_limit = 1.0
rate_burst = 2

[mcp]
upstream = "https://game.spacemolt.com/mcp"
upstream_version = "v0.43.0"
